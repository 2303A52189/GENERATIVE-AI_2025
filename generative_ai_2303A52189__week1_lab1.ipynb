{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4msSadpy4ZuFhdLdTUFWk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52189/GENERATIVE-AI_2025/blob/main/generative_ai_2303A52189__week1_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries\n",
        "\n",
        "Y Actual YP pred\n",
        "\n",
        "20 20.5\n",
        "\n",
        "30 30.3\n",
        "\n",
        "40 40.2\n",
        "\n",
        "50 50.6\n",
        "\n",
        "60 60.7\n",
        "\n",
        "Tabela 1: YActual Vs. YP red\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QVyjGUdbpa9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n"
      ],
      "metadata": {
        "id": "oaVUYv2KkZDN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_actual = np.array([20, 30, 40, 50, 60])\n",
        "y_pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n"
      ],
      "metadata": {
        "id": "s0VpXliAk3yC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mae = mean_absolute_error(y_actual, y_pred)  # Mean Absolute Error\n",
        "mse = mean_squared_error(y_actual, y_pred)  # Mean Squared Error\n",
        "rmse = sqrt(mse)  # Root Mean Squared Error"
      ],
      "metadata": {
        "id": "3i7tpj1nlG2v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81y_NhculKDv",
        "outputId": "5f824a6f-54b9-4f42-8720-f919ac201fad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = y_actual.reshape(-1, 1)  # Reshape data for scikit-learn (2D array for the independent variable)\n",
        "y = y_pred  # Target (dependent variable)\n"
      ],
      "metadata": {
        "id": "NtnVH6JblNAy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "# Print the model's coefficients and intercept\n",
        "print(f'Linear Regression Coefficients: {model.coef_}')\n",
        "print(f'Linear Regression Intercept: {model.intercept_}')\n",
        "\n",
        "# Predict using the linear regression model\n",
        "y_pred_lr = model.predict(X)\n",
        "\n",
        "# Print predicted values from the linear regression model\n",
        "print(f'Predicted values using Linear Regression: {y_pred_lr}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acJvIULlQdv",
        "outputId": "cba73bba-b0d3-460a-ef42-d1dddc7e1300"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Coefficients: [1.007]\n",
            "Linear Regression Intercept: 0.17999999999999972\n",
            "Predicted values using Linear Regression: [20.32 30.39 40.46 50.53 60.6 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 (1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "\n",
        "YActual YP red\n",
        "\n",
        "0 0 1 1 2 0\n",
        "\n",
        "0 0 1 0 2 0\n",
        "\n",
        "0 1 1 2 2 1\n",
        "\n",
        "0 2 1 0 2 2\n",
        "\n",
        "0 2 1 2 2 2\n",
        "\n",
        "Tabela 2: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "rLUU9h1WpoIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Data (Actual vs Predicted)\n",
        "Y_actual = np.array([0, 0, 1, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2])\n",
        "Y_pred = np.array([0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 1, 2, 2, 2])\n",
        "\n"
      ],
      "metadata": {
        "id": "nLGolR0SnbKJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tp, fp, fn = {0: 0, 1: 0, 2: 0}, {0: 0, 1: 0, 2: 0}, {0: 0, 1: 0, 2: 0}\n",
        "for i in range(len(Y_actual)):\n",
        "    for cls in [0, 1, 2]:\n",
        "        if Y_actual[i] == cls and Y_pred[i] == cls: tp[cls] += 1\n",
        "        elif Y_actual[i] != cls and Y_pred[i] == cls: fp[cls] += 1\n",
        "        elif Y_actual[i] == cls and Y_pred[i] != cls: fn[cls] += 1\n",
        "\n",
        "accuracy_scratch = np.sum(Y_actual == Y_pred) / len(Y_actual)\n",
        "precision_scratch = {cls: tp[cls] / (tp[cls] + fp[cls]) for cls in [0, 1, 2]}\n",
        "recall_scratch = {cls: tp[cls] / (tp[cls] + fn[cls]) for cls in [0, 1, 2]}\n",
        "f1_scratch = {cls: 2 * precision_scratch[cls] * recall_scratch[cls] / (precision_scratch[cls] + recall_scratch[cls]) for cls in [0, 1, 2]}\n"
      ],
      "metadata": {
        "id": "IDjM1c_on1WI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display manual metrics\n",
        "print(\"Metrics from scratch:\")\n",
        "print(f\"Accuracy: {accuracy_scratch}\")\n",
        "for cls in [0, 1, 2]:\n",
        "    print(f\"Class {cls} - Precision: {precision_scratch[cls]}, Recall: {recall_scratch[cls]}, F1-Score: {f1_scratch[cls]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0M_UnbGoJNR",
        "outputId": "5e7b0668-d80f-4697-ab6d-8f36ca5a1b86"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics from scratch:\n",
            "Accuracy: 0.7333333333333333\n",
            "Class 0 - Precision: 0.7777777777777778, Recall: 0.6363636363636364, F1-Score: 0.7000000000000001\n",
            "Class 1 - Precision: 0.7142857142857143, Recall: 0.625, F1-Score: 0.6666666666666666\n",
            "Class 2 - Precision: 0.7142857142857143, Recall: 0.9090909090909091, F1-Score: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Library metrics calculation\n",
        "accuracy_lib = accuracy_score(Y_actual, Y_pred)\n",
        "precision_lib = precision_score(Y_actual, Y_pred, average=None)\n",
        "recall_lib = recall_score(Y_actual, Y_pred, average=None)\n",
        "f1_lib = f1_score(Y_actual, Y_pred, average=None)"
      ],
      "metadata": {
        "id": "O9iwM1Ebok2q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display library metrics\n",
        "print(\"\\nMetrics from libraries:\")\n",
        "print(f\"Accuracy: {accuracy_lib}\")\n",
        "for cls, (p, r, f1) in enumerate(zip(precision_lib, recall_lib, f1_lib)):\n",
        "    print(f\"Class {cls} - Precision: {p}, Recall: {r}, F1-Score: {f1}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(Y_actual, Y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hlTJ011oo0C",
        "outputId": "c8e262a9-c2cf-4dba-e092-89a52d7be686"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics from libraries:\n",
            "Accuracy: 0.7333333333333333\n",
            "Class 0 - Precision: 0.7777777777777778, Recall: 0.6363636363636364, F1-Score: 0.7\n",
            "Class 1 - Precision: 0.7142857142857143, Recall: 0.625, F1-Score: 0.6666666666666666\n",
            "Class 2 - Precision: 0.7142857142857143, Recall: 0.9090909090909091, F1-Score: 0.8\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.64      0.70        11\n",
            "           1       0.71      0.62      0.67         8\n",
            "           2       0.71      0.91      0.80        11\n",
            "\n",
            "    accuracy                           0.73        30\n",
            "   macro avg       0.74      0.72      0.72        30\n",
            "weighted avg       0.74      0.73      0.73        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}